{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c58c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (precision_score, recall_score, f1_score, roc_auc_score,\n",
    "                             average_precision_score, precision_recall_curve, roc_curve,\n",
    "                             confusion_matrix, classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45649d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load models and test data from 02_modeling notebook\n",
    "# Run this after executing 02_modeling.ipynb\n",
    "\n",
    "# These variables come from 02_modeling.ipynb - run that notebook first\n",
    "# log_reg, dt, rf, svm, gb, X_test_clean, y_test\n",
    "\n",
    "print(\"Make sure you have run 02_modeling.ipynb first!\")\n",
    "print(f\"Test set size: {X_test_clean.shape}\")\n",
    "print(f\"Test label distribution:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Evaluation Function\n",
    "\n",
    "def evaluate(model, X_test, y_test, model_name=\"Model\"):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} Evaluation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"ROC-AUC:   {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    print(f\"PR-AUC:    {average_precision_score(y_test, y_prob):.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "    print(f\"\\n{classification_report(y_test, y_pred)}\")\n",
    "\n",
    "    # Plot curves side by side\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    axes[0].plot(fpr, tpr)\n",
    "    axes[0].set_title(f\"ROC Curve — {model_name}\")\n",
    "    axes[0].set_xlabel(\"FPR\"); axes[0].set_ylabel(\"TPR\")\n",
    "\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    axes[1].plot(rec, prec)\n",
    "    axes[1].set_title(f\"PR Curve — {model_name}\")\n",
    "    axes[1].set_xlabel(\"Recall\"); axes[1].set_ylabel(\"Precision\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e0c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(log_reg, X_test_clean, y_test, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(dt, X_test_clean, y_test, \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69edb8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rf, X_test_clean, y_test, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91afe028",
   "metadata": {},
   "source": [
    "## 4. SVM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(svm, X_test_clean, y_test, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd3ff8",
   "metadata": {},
   "source": [
    "## 5. Gradient Boosting Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b21b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(gb, X_test_clean, y_test, \"Gradient Boosting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e4b1ea",
   "metadata": {},
   "source": [
    "## Model Comparison Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfed5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Report for All 5 Models\n",
    "\n",
    "def generate_report(models_dict, X_test, y_test):\n",
    "    \"\"\"Generate a comparison report for multiple models.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, model in models_dict.items():\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        results.append({\n",
    "            \"Model\": name,\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1 Score\": f1_score(y_test, y_pred),\n",
    "            \"ROC-AUC\": roc_auc_score(y_test, y_prob),\n",
    "            \"PR-AUC\": average_precision_score(y_test, y_prob)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).round(4)\n",
    "\n",
    "# All 5 models\n",
    "report_models = {\n",
    "    \"Logistic Regression\": log_reg,\n",
    "    \"Decision Tree\": dt,\n",
    "    \"Random Forest\": rf,\n",
    "    \"SVM\": svm,\n",
    "    \"Gradient Boosting\": gb\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON REPORT (All 5 Models)\")\n",
    "print(\"=\" * 60)\n",
    "report_df = generate_report(report_models, X_test_clean, y_test)\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10781687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC and PR Curves Comparison\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for name, model in report_models.items():\n",
    "    y_prob = model.predict_proba(X_test_clean)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curves Comparison\")\n",
    "plt.legend(loc=\"lower right\", fontsize=8)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for name, model in report_models.items():\n",
    "    y_prob = model.predict_proba(X_test_clean)[:, 1]\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    pr_auc = average_precision_score(y_test, y_prob)\n",
    "    plt.plot(rec, prec, label=f\"{name} (PR-AUC={pr_auc:.3f})\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curves Comparison\")\n",
    "plt.legend(loc=\"lower left\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
