ğŸ¯ Project Objective

The goal is to build a model that:

Detects potentially fraudulent providers

Handles heavy class imbalance (~10% fraud cases)

Produces interpretable results for investigators

Prioritizes high-risk providers to reduce investigation overhead

This includes full justification of data prep, modeling choices, evaluation, and error analysis.


GIU_2620_67_26747_2025-11-10T21â€¦

ğŸ§  Dataset Description

Source: Kaggle â€“ Healthcare Provider Fraud Detection Analysis

Included Files:

Train_Beneficiarydata.csv â€“ patient demographics & chronic conditions

Train_Inpatientdata.csv â€“ hospital admissions, procedures, costs

Train_Outpatientdata.csv â€“ outpatient visits & tests

Train_labels.csv â€“ provider fraud labels (Yes / No)

Key Identifiers:

BeneID â†’ links patients to claims

Provider â†’ links claims to final fraud label

GIU_2620_67_26747_2025-11-10T21â€¦

ğŸ›  Pipeline Breakdown
1. ğŸ” Data Understanding & Exploration

In 01_data_exploration_and_feature_engineering.ipynb, we:

Join multi-table data using BeneID and Provider

Assess missing values & inconsistencies

Do EDA on patients, claims, and providers

Compare fraud vs non-fraud behavior patterns

Visualize class distribution, claim amounts, correlations, etc.

We also engineer provider-level aggregated features such as:

Claim counts

Average claim amounts

Percentage of chronic conditions

Cost ratios

Visit/procedure patterns


GIU_2620_67_26747_2025-11-10T21â€¦

2. âš– Handling Class Imbalance

Fraud cases â‰ˆ 10% â†’ heavily skewed dataset.
Our notebook compares multiple strategies:

Class weighting

Oversampling (SMOTE)

Undersampling

Cost-sensitive learning

Metrics prioritized:

Precision

Recall

F1-score

PR-AUC

Accuracy is not used as a reliable indicator.


GIU_2620_67_26747_2025-11-10T21â€¦

3. ğŸ¤– Modeling

Implemented in 02_modeling.ipynb.

Primary model options examined:

Logistic Regression (baseline, interpretable)

Random Forest (robust for tabular mixed data)

Gradient Boosting (XGBoost/LightGBM for best performance)

SVM (tested but limited for large + imbalanced datasets)

We justify each algorithm based on:

Interpretability

Performance on imbalanced data

Computational cost

Real-world applicability


GIU_2620_67_26747_2025-11-10T21â€¦

4. ğŸ“‰ Model Evaluation

In 03_evaluation.ipynb, we run:

Cross-validation

ROC-AUC and PR-AUC

Confusion matrices

Precision-Recall curves

We also include an error analysis section with:

Case studies of false positives

Case studies of false negatives

Discussion of why the model failed in each scenario

Suggestions for future improvements


GIU_2620_67_26747_2025-11-10T21â€¦

ğŸ“ How to Run the Project
1. Clone the repo
git clone https://github.com/<your-username>/fraud_detection_project.git
cd fraud_detection_project

2. Install dependencies
pip install -r requirements.txt

3. Add dataset

Place the four CSV files under data/
(or run the download instructions if included).

4. Run the notebooks in order:

01_data_exploration_and_feature_engineering.ipynb

02_modeling.ipynb

03_evaluation.ipynb

ğŸ“Š Results Summary

(Youâ€™ll fill this section after finishing your results)
Example template:

Best Model: Random Forest / XGBoost / Logistic Regression

Best Metric (e.g., PR-AUC): 0.87

False Positive Patterns: Overbilling, high claim frequency

False Negative Patterns: Low-amount but suspicious patterns missed

Business Impact: Reduces investigation workload by X%

ğŸ‘¥ Team Members

(Add your team names)

ğŸ Final Deliverables

âœ” Structured GitHub repo
âœ” Full technical report
âœ” Presentation slides (~10 minutes)
âœ” Reproducible notebooks
âœ” Clear modeling justification and error analysis

If you want, I can also generate:
âœ¨ requirements.txt
âœ¨ A polished technical report template
âœ¨ Your full presentation structure + slides outline
âœ¨ A more aesthetic version of the README with banners, icons, and emojis

Just say the word.
