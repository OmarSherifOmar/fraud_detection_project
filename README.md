

# ğŸ¥ Healthcare Provider Fraud Detection

*Machine Learning Project â€“ GIU*

---

## ğŸ¯ **Project Objective**

The goal of this project is to build a machine learning pipeline that:

* ğŸ” **Detects potentially fraudulent healthcare providers**
* âš–ï¸ **Handles heavy class imbalance** (~10% fraud cases)
* ğŸ” **Produces interpretable results** for investigators
* ğŸš¨ **Prioritizes high-risk providers** to reduce investigation workloads
* ğŸ“ Provides **full justification** for data preparation, modeling, evaluation, and error analysis

---

## ğŸ§  **Dataset Description**

**Source:** Kaggle â€“ *Healthcare Provider Fraud Detection Analysis*

### **Included Files**

| File                        | Description                                   |
| --------------------------- | --------------------------------------------- |
| `Train_Beneficiarydata.csv` | Patient demographics, chronic conditions      |
| `Train_Inpatientdata.csv`   | Inpatient admissions, procedures, claim costs |
| `Train_Outpatientdata.csv`  | Outpatient visits & tests                     |
| `Train_Labels.csv`          | Provider-level fraud labels (**Yes / No**)    |

### ğŸ”— **Key Identifiers**

* **BeneID** â†’ links patients to claims
* **Provider** â†’ links claims to final fraud label

---

## ğŸ›  **Pipeline Overview**

### **1. ğŸ” Data Understanding & Exploration**

*Notebook: `01_data_exploration_and_feature_engineering.ipynb`*

Key steps:

* Merging multiple tables using `BeneID` and `Provider`
* Checking missing values and inconsistencies
* Exploring patient demographics, inpatient/outpatient claims
* Comparing behavior patterns between **fraud** and **non-fraud** providers
* Visualizing distributions, correlations, and cost behavior

**Engineered provider-level features include:**

* Total claim counts
* Average / max claim amounts
* Percentage of chronic conditions
* Visit/procedure patterns
* Cost ratios
* Patient distribution metrics

---

### **2. âš– Handling Class Imbalance**

Fraud â‰ˆ **10%** â†’ highly imbalanced dataset

Techniques evaluated:

* ğŸ§® **Class weighting**
* â• **SMOTE oversampling**
* â– **Random undersampling**
* ğŸ’° **Cost-sensitive learning**

**Priority Metrics:**

* Precision
* Recall
* F1-Score
* **PR-AUC** (best for imbalance)
* *(Accuracy avoided due to misleading results)*

---

### **3. ğŸ¤– Modeling**

*Notebook: `02_modeling.ipynb`*

Models evaluated:

| Model                                      | Reason                                          |
| ------------------------------------------ | ----------------------------------------------- |
| **Logistic Regression**                    | Baseline, interpretable                         |
| **Random Forest**                          | Handles tabular + mixed data well               |
| **Gradient Boosting (XGBoost / LightGBM)** | Best performance, handles imbalance             |
| **SVM**                                    | Tested, but high cost for large imbalanced data |

Evaluation based on:

* Interpretability
* Performance on imbalanced data
* Training speed
* Real-world deployability

---

### **4. ğŸ“‰ Model Evaluation & Error Analysis**

*Notebook: `03_evaluation.ipynb`*

Includes:

* Cross-validation
* ROC-AUC & PR-AUC
* Confusion matrices
* Precision-Recall curves

**Error Analysis:**

* ğŸ”´ **False Positives:**
  Often due to high billing frequency or unusual cost spikes
* ğŸŸ  **False Negatives:**
  Low-amount claims with subtle suspicious patterns
* ğŸ”§ **Recommendations:**
  Feature expansion, anomaly detection, cost-benefit thresholds


## ğŸ **Final Deliverables**

* âœ” Reproducible notebooks
* âœ” Clean, structured GitHub repo
* âœ” Full technical report
* âœ” Error analysis
* âœ” Presentation slides (10 minutes)


